# ğŸ›¡ï¸ AI Behavioral Firewall

An intelligent, real-time system that **intercepts and filters potentially unsafe, toxic, or biased AI outputs** using Natural Language Processing (NLP) and Deep Learning techniques. Built using FastAPI, Detoxify, and spaCy.

---

## ğŸš€ Features

- âœ… Detects toxicity using the [Detoxify](https://github.com/unitaryai/detoxify) model
- âœ… Redacts sensitive PII (Personal Identifiable Information) like names, locations, organizations using spaCy
- âœ… Logs every intercepted AI output with a timestamp for auditing
- âœ… Provides a RESTful API with interactive Swagger UI via FastAPI `/docs`
- âœ… Scalable backend architecture for integration with AI/ML pipelines

---

## ğŸ“¦ Tech Stack

- **Python 3.10**
- **FastAPI** â€“ Web framework for backend API
- **Detoxify** â€“ Toxicity detection using BERT-based model
- **spaCy** â€“ NLP library for entity recognition and redaction
- **Uvicorn** â€“ Lightning-fast ASGI server
- **Git & GitHub** â€“ Version control and collaboration

---

## ğŸ§  How It Works

1. A client or user sends AI-generated output to the `/intercept/` endpoint.
2. The backend:
   - Evaluates the output for toxicity using Detoxify.
   - If the output is toxic, it redacts sensitive info (names, places, orgs, etc.).
   - Saves the original and filtered outputs to a log file (`logs/intercepts_log.json`).
3. The API returns the filtered (safe) output along with a detailed report.

---

## ğŸ“‚ Project Structure

AI-Behavioral-Firewall-/
â”œâ”€â”€ .gitignore # Git ignore rules
â”œâ”€â”€ main.py # FastAPI app (API logic and routing)
â”œâ”€â”€ firewall.py # Core functions: toxicity detection, redaction, logging
â”œâ”€â”€ logs/ # Auto-created folder to store intercepted logs
â”œâ”€â”€ venv310/ # Python virtual environment (excluded from Git)
â”œâ”€â”€ README.md # Project overview and documentation
â””â”€â”€ ui/ # (Coming soon) Simple UI for interaction
